{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9bc8289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  date                               chat        sender  \\\n",
      "0  2025-06-24 09:05:42                    áˆá‹‹áˆ­á‹«á‹Š áˆ˜áˆáˆ¶á‰½ Chat          áŠá‚ ğŸ¥€   \n",
      "1  2025-06-24 09:06:48                    áˆá‹‹áˆ­á‹«á‹Š áˆ˜áˆáˆ¶á‰½ Chat     mekutii12   \n",
      "2  2025-06-24 09:07:31  Tulips Event & sales ğŸ¤ Xpand SMMA   TulipsEvent   \n",
      "3  2025-06-24 09:09:08  Tulips Event & sales ğŸ¤ Xpand SMMA   TulipsEvent   \n",
      "4  2025-06-24 09:09:08                    áˆá‹‹áˆ­á‹«á‹Š áˆ˜áˆáˆ¶á‰½ Chat  mklldouble_g   \n",
      "\n",
      "                                             message  \\\n",
      "0  á‰ áŒˆáŠá‰µ á‰ áŠá‰ áˆ¨á‰½ á‰ á‹•á€ áˆ…á‹­á‹á‰µ áˆáŠ•á‰³  á‰ áˆá‹µáˆ­ á‰°á‰°áŠ¨áˆˆá‰½ áˆá‰µáˆ†áŠá‹ áˆˆáŠ á‹³áˆ...   \n",
      "1  áŠ¥áŠ“á‰µ áŠ áŒáŠá‰¼áˆ»áˆˆá‹ áŠ¨áˆ˜áˆµá‰€áˆ‰ áŠ¥áŒáˆ­áŒŒ áˆµáˆŸáŠ• áŠ¥á‹¨áŒ áˆ«áˆ áŠ¨á áŠ¥áŠ•á‹²áˆ áˆ›á‹•áˆ¨áŒŒ ...   \n",
      "2  #NewArrival ğŸ˜Ladies Gift Combo ğŸ˜  Commission (...   \n",
      "3   ğŸŸ¥Gucci ladies bag light brown color ğŸŸ¥ Sold out ğŸŸ¥   \n",
      "4                                     áŠ¥áˆ¨áá‰µ áŠ á‰³áˆµá‹±áˆ áŠ¥áŠ•á‹´   \n",
      "\n",
      "                                     cleaned_message  \\\n",
      "0  á‰ áŒˆáŠá‰µ á‰ áŠá‰ áˆ¨á‰½ á‰ á‹•á€ áˆ…á‹­á‹á‰µ áˆáŠ•á‰³ á‰ áˆá‹µáˆ­ á‰°á‰°áŠ¨áˆˆá‰½ áˆá‰µáˆ†áŠá‹ áˆˆáŠ á‹³áˆ ...   \n",
      "1  áŠ¥áŠ“á‰µ áŠ áŒáŠá‰¼áˆ»áˆˆá‹ áŠ¨áˆ˜áˆµá‰€áˆ‰ áŠ¥áŒáˆ­áŒŒ áˆµáˆŸáŠ• áŠ¥á‹¨áŒ áˆ«áˆ áŠ¨á áŠ¥áŠ•á‹²áˆ áˆ›á‹•áˆ¨áŒŒ ...   \n",
      "2  newarrival ladies gift combo  commission áŠ®áˆšáˆ½áŠ• ...   \n",
      "3       gucci ladies bag light brown color  sold out   \n",
      "4                                     áŠ¥áˆ¨áá‰µ áŠ á‰³áˆµá‹±áˆ áŠ¥áŠ•á‹´   \n",
      "\n",
      "                                              tokens  \n",
      "0  á‰ áŒˆáŠá‰µ á‰ áŠá‰ áˆ¨á‰½ á‰ á‹•á€ áˆ…á‹­á‹á‰µ áˆáŠ•á‰³ á‰ áˆá‹µáˆ­ á‰°á‰°áŠ¨áˆˆá‰½ áˆá‰µáˆ†áŠá‹ áˆˆáŠ á‹³áˆ ...  \n",
      "1  áŠ¥áŠ“á‰µ áŠ áŒáŠá‰¼áˆ»áˆˆá‹ áŠ¨áˆ˜áˆµá‰€áˆ‰ áŠ¥áŒáˆ­áŒŒ áˆµáˆŸáŠ• áŠ¥á‹¨áŒ áˆ«áˆ áŠ¨á áŠ¥áŠ•á‹²áˆ áˆ›á‹•áˆ¨áŒŒ ...  \n",
      "2  newarrival ladies gift combo commission áŠ®áˆšáˆ½áŠ• 5...  \n",
      "3        gucci ladies bag light brown color sold out  \n",
      "4                                     áŠ¥áˆ¨áá‰µ áŠ á‰³áˆµá‹±áˆ áŠ¥áŠ•á‹´  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your cleaned CSV with tokenized messages\n",
    "df = pd.read_csv(\"data/messages.csv\", encoding=\"utf-8\")\n",
    "\n",
    "print(df.head())  # Inspect first rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193e5e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             message  \\\n",
      "0  á‰ áŒˆáŠá‰µ á‰ áŠá‰ áˆ¨á‰½ á‰ á‹•á€ áˆ…á‹­á‹á‰µ áˆáŠ•á‰³  á‰ áˆá‹µáˆ­ á‰°á‰°áŠ¨áˆˆá‰½ áˆá‰µáˆ†áŠá‹ áˆˆáŠ á‹³áˆ...   \n",
      "1  áŠ¥áŠ“á‰µ áŠ áŒáŠá‰¼áˆ»áˆˆá‹ áŠ¨áˆ˜áˆµá‰€áˆ‰ áŠ¥áŒáˆ­áŒŒ áˆµáˆŸáŠ• áŠ¥á‹¨áŒ áˆ«áˆ áŠ¨á áŠ¥áŠ•á‹²áˆ áˆ›á‹•áˆ¨áŒŒ ...   \n",
      "2  #NewArrival ğŸ˜Ladies Gift Combo ğŸ˜  Commission (...   \n",
      "3   ğŸŸ¥Gucci ladies bag light brown color ğŸŸ¥ Sold out ğŸŸ¥   \n",
      "4                                     áŠ¥áˆ¨áá‰µ áŠ á‰³áˆµá‹±áˆ áŠ¥áŠ•á‹´   \n",
      "\n",
      "                                     cleaned_message  \\\n",
      "0  á‰ áŒˆáŠá‰µ á‰ áŠá‰ áˆ¨á‰½ á‰ á‹•á€ áˆ…á‹­á‹á‰µ áˆáŠ•á‰³  á‰ áˆá‹µáˆ­ á‰°á‰°áŠ¨áˆˆá‰½ áˆá‰µáˆ†áŠá‹ áˆˆáŠ á‹³áˆ...   \n",
      "1  áŠ¥áŠ“á‰µ áŠ áŒáŠá‰¼áˆ»áˆˆá‹ áŠ¨áˆ˜áˆµá‰€áˆ‰ áŠ¥áŒáˆ­áŒŒ áˆµáˆŸáŠ• áŠ¥á‹¨áŒ áˆ«áˆ áŠ¨á áŠ¥áŠ•á‹²áˆ áˆ›á‹•áˆ¨áŒŒ ...   \n",
      "2  newarrival ladies gift combo   commission áŠ®áˆšáˆ½áŠ•...   \n",
      "3      gucci ladies bag light brown color  sold out    \n",
      "4                                     áŠ¥áˆ¨áá‰µ áŠ á‰³áˆµá‹±áˆ áŠ¥áŠ•á‹´   \n",
      "\n",
      "                                              tokens  \n",
      "0  [á‰ áŒˆáŠá‰µ, á‰ áŠá‰ áˆ¨á‰½, á‰ á‹•á€, áˆ…á‹­á‹á‰µ, áˆáŠ•á‰³, á‰ áˆá‹µáˆ­, á‰°á‰°áŠ¨áˆˆá‰½, áˆá‰µáˆ†...  \n",
      "1  [áŠ¥áŠ“á‰µ, áŠ áŒáŠá‰¼áˆ»áˆˆá‹, áŠ¨áˆ˜áˆµá‰€áˆ‰, áŠ¥áŒáˆ­áŒŒ, áˆµáˆŸáŠ•, áŠ¥á‹¨áŒ áˆ«áˆ, áŠ¨á, áŠ¥áŠ•...  \n",
      "2  [newarrival, ladies, gift, combo, commission, ...  \n",
      "3  [gucci, ladies, bag, light, brown, color, sold...  \n",
      "4                                 [áŠ¥áˆ¨áá‰µ, áŠ á‰³áˆµá‹±áˆ, áŠ¥áŠ•á‹´]  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # Lowercase (if applicable) & remove unwanted characters\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Example: preprocess and tokenize a column\n",
    "df['cleaned_message'] = df['message'].apply(clean_text)\n",
    "df['tokens'] = df['cleaned_message'].apply(tokenize)\n",
    "\n",
    "print(df[['message', 'cleaned_message', 'tokens']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c68638",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/ner_raw.conll\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for tokens in df[\"tokens\"]:\n",
    "        for token in tokens:\n",
    "            f.write(f\"{token} O\\n\")\n",
    "        f.write(\"\\n\")  # Blank line between messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac19f1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated data saved to data/ner_annotated.conll\n"
     ]
    }
   ],
   "source": [
    "# Example annotated data format:\n",
    "# A list of sentences, each sentence is a list of (token, label) tuples\n",
    "annotated_data = [\n",
    "    [(\"áŠ ááˆªáŠ«\", \"B-ORG\"), (\"áˆ“á‹¨á‰°á‹¨áŒáˆ›áˆ…á‰ áˆ­\", \"I-ORG\"), (\"áˆ›á‰¥á‰…á‹«\", \"O\"), (\"á‰€áŠ•\", \"O\")],\n",
    "    [(\"áŠ á‹³áˆ›\", \"B-LOC\"), (\"áŠ¨á‰°áˆ›\", \"I-LOC\")],\n",
    "    [(\"á‰´áˆµá‹\", \"B-PER\"), (\"á‰£á‰½\", \"I-PER\")]\n",
    "]\n",
    "\n",
    "output_path = \"data/ner_annotated.conll\"\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sentence in annotated_data:\n",
    "        for token, label in sentence:\n",
    "            f.write(f\"{token} {label}\\n\")\n",
    "        f.write(\"\\n\")  # Blank line to separate sentences\n",
    "\n",
    "print(f\"Annotated data saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "068a2184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted CoNLL to spaCy format\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "\n",
    "def read_conll(filepath):\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read().strip().split(\"\\n\\n\")\n",
    "    data = []\n",
    "    for block in lines:\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for line in block.split(\"\\n\"):\n",
    "            token, tag = line.split()\n",
    "            tokens.append(token)\n",
    "            tags.append(tag)\n",
    "        data.append((tokens, tags))\n",
    "    return data\n",
    "\n",
    "def conll_to_spacy(data, nlp):\n",
    "    doc_bin = DocBin()\n",
    "    for tokens, tags in data:\n",
    "        doc = spacy.tokens.Doc(nlp.vocab, words=tokens)\n",
    "        ents = []\n",
    "        start = None\n",
    "        end = None\n",
    "        label = None\n",
    "        for i, tag in enumerate(tags):\n",
    "            if tag.startswith(\"B-\"):\n",
    "                if start is not None:\n",
    "                    ents.append(spacy.tokens.Span(doc, start, end, label=label))\n",
    "                start = i\n",
    "                end = i + 1\n",
    "                label = tag[2:]\n",
    "            elif tag.startswith(\"I-\") and start is not None:\n",
    "                end = i + 1\n",
    "            else:\n",
    "                if start is not None:\n",
    "                    ents.append(spacy.tokens.Span(doc, start, end, label=label))\n",
    "                    start = None\n",
    "                    end = None\n",
    "                    label = None\n",
    "        # catch last entity\n",
    "        if start is not None:\n",
    "            ents.append(spacy.tokens.Span(doc, start, end, label=label))\n",
    "        doc.ents = ents\n",
    "        doc_bin.add(doc)\n",
    "    return doc_bin\n",
    "\n",
    "nlp = spacy.blank(\"am\")  # Assuming Amharic language model, or replace with \"en\" if English\n",
    "data = read_conll(\"data/ner_annotated.conll\")\n",
    "doc_bin = conll_to_spacy(data, nlp)\n",
    "doc_bin.to_disk(\"data/train.spacy\")\n",
    "print(\"Converted CoNLL to spaCy format\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
